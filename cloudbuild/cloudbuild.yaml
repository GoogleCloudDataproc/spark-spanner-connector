steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-spark-spanner-connector-presubmit', '-f', 'cloudbuild/Dockerfile', '.']

  # 2. Fetch maven and dependencies
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-spanner-connector-presubmit'
    id: 'init'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'init']

  # 3. Run integration tests with Spanner emulator
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-spanner-connector-presubmit'
    id: 'integration-real-spanner'
    waitFor: ['init']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'integrationtest-real-spanner']
    env:
      - 'SPANNER_PROJECT_ID=$PROJECT_ID'
      - 'SPANNER_INSTANCE_ID=test-instance'
      - 'SPANNER_DATABASE_ID=testdb'

  # 4. Run acceptance tests by creating real Dataproc clusters.
  # TODO: Make the acceptance test run in parallel with integration-real-spanner.
  - name: 'gcr.io/$PROJECT_ID/dataproc-spark-spanner-connector-presubmit'
    id: 'acceptance-test'
    waitFor: ['init']
    entrypoint: 'bash'
    args: ['/workspace/cloudbuild/presubmit.sh', 'acceptance-test']
    env:
      - 'SPANNER_PROJECT_ID=$PROJECT_ID'
      - 'GOOGLE_CLOUD_PROJECT=$PROJECT_ID'
      - 'SPANNER_INSTANCE_ID=accept-testins'
      - 'SPANNER_DATABASE_ID=accept-testdb'
      - 'ACCEPTANCE_TEST_BUCKET=spark-spanner-connector-acceptance-test'


timeout: 3600s
options:
  machineType: 'N1_HIGHCPU_32'
